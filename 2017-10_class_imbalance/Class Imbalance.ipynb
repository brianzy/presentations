{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Welcome to Norfolk Data Science\n",
    "\n",
    "<img src=\"https://secure.meetupstatic.com/photos/event/c/f/b/a/highres_452453178.jpeg\">\n",
    "\n",
    "### Why class imbalances ruin predictions and how to remedy\n",
    "* 2017-10-03\n",
    "* Christopher Brossman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the class imbalance problem?\n",
    "\n",
    "<img src=\"http://sci2s.ugr.es/sites/default/files/files/ComplementaryMaterial/imbalanced/yeast4_s1.0tr_mcg_vs_gvh.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lets look at some real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#metrics to print\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# percision recall curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1) #to reproduce results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pct Minority: 9.09%\n",
      "Pct Majority: 90.91%\n"
     ]
    }
   ],
   "source": [
    "#use Iris data and pick one flower to filter down\n",
    "# currently each has 50\n",
    "col = ['sepal_length','sepal_width','petal_length','petal_width','type']\n",
    "data = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', names = col)\n",
    "\n",
    "#pick a flower and select 10 out of the 50 observations\n",
    "virginica = data[data.type == 'Iris-virginica'].sample(frac=0.2).copy()\n",
    "not_virginica = data[data.type != 'Iris-virginica']\n",
    "df = pd.concat([virginica,not_virginica])\n",
    "#turn into binary \n",
    "df['virginica'] = np.where(df['type']=='Iris-virginica', 1, 0)\n",
    "df.drop('type',inplace=True, axis=1)\n",
    "print('Pct Minority: ' + str(round((df.virginica.sum()/df.virginica.count())*100,2)) + '%')\n",
    "print('Pct Majority: ' + str(round((1-df.virginica.sum()/df.virginica.count())*100,2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation(y,y_prob,ztype):\n",
    "    y_class = np.where(y_prob > .5,1,0)\n",
    "    acc = metrics.accuracy_score(y, y_class)\n",
    "    f1 = metrics.f1_score(y, y_class)\n",
    "    pre = precision_score(y,y_class)\n",
    "    rec = recall_score(y,y_class)\n",
    "    \n",
    "    print('Evaluation for ' + ztype)\n",
    "    print('Accuracy : ', str(round(acc,4)))\n",
    "    print('F1       : ', str(round(f1,4)))\n",
    "    print('Precision: ', str(round(pre,4)))\n",
    "    print('Recall   : ', str(round(rec,4)))\n",
    "    print()\n",
    "    print(confusion_matrix(y, y_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imbalance is Common, and Accuracy is NOT the right metric\n",
    "\n",
    "* In prior example 9% of classes were \"virginica\" and 91% were \"not virginica\"\n",
    "* If we predicted all classes were \"not virginica\" we would have 91% accuracy!\n",
    "* Your classifier may be doing this!\n",
    "\n",
    "... but the accuracy... it is like a paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can you collect more data?\n",
    "\n",
    "You might think it’s silly, but collecting more data is almost always overlooked.\n",
    "\n",
    "Can you collect more data? Take a second and think about whether you are able to gather more data on your problem.\n",
    "\n",
    "A larger dataset might expose a different and perhaps more balanced perspective on the classes.\n",
    "\n",
    "More examples of minor classes may be useful later when we look at resampling your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try Changing Your Performance Metric\n",
    "\n",
    "As mentioned prior - accuracy is a paradox and no longer the appropriate measurement. \n",
    "\n",
    "* F1 Score (or F-score): A weighted average of precision and recall. - probably the single best measurement\n",
    "    * Precision: A measure of a classifiers exactness.\n",
    "    * Recall: A measure of a classifiers completeness\n",
    "* Confusion Matrix -- always check the confusion matrix!\n",
    "\n",
    "### Also check out these metrics\n",
    "* Kappa (or Cohen’s kappa): Classification accuracy normalized by the imbalance of the classes in the data.\n",
    "* ROC Curves: either traditional OR precision/recall ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for training set\n",
      "Accuracy :  0.9351\n",
      "F1       :  0.0\n",
      "Precision:  0.0\n",
      "Recall   :  0.0\n",
      "\n",
      "[[72  0]\n",
      " [ 5  0]]\n",
      "\n",
      "Evaluation for testing set\n",
      "Accuracy :  0.8485\n",
      "F1       :  0.0\n",
      "Precision:  0.0\n",
      "Recall   :  0.0\n",
      "\n",
      "[[28  0]\n",
      " [ 5  0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df.drop('virginica',axis=1), df.virginica, test_size=0.3,random_state=0)\n",
    "print(\"virginica in train set = \", str(y_train.sum()))\n",
    "print(\"virginica in dev set = \", str(y_dev.sum()))\n",
    "print()\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#Get predicted classes\n",
    "y_train_pred = logistic.predict_proba(X_train)[:,1]\n",
    "y_dev_pred = logistic.predict_proba(X_dev)[:,1]\n",
    "\n",
    "evaluation(y_train,y_train_pred,'training set')\n",
    "print()\n",
    "evaluation(y_dev,y_dev_pred,'testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try Resampling Your Dataset\n",
    "\n",
    "Risk if undersample\n",
    "<img src=\"http://www.chioka.in/wp-content/uploads/2013/08/Undersampling.png\">\n",
    "\n",
    "Risk if oversample\n",
    "<img src=\"http://www.chioka.in/wp-content/uploads/2013/08/Oversampling.png\">\n",
    "\n",
    "Some Rules of Thumb\n",
    "\n",
    "* Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)\n",
    "* Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less)\n",
    "* Consider testing random and non-random (e.g. stratified) sampling schemes.\n",
    "* Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginica in train set =  7\n",
      "virginica in dev set =  3\n",
      "Evaluation for training set\n",
      "Accuracy :  0.9286\n",
      "F1       :  0.7692\n",
      "Precision:  0.8333\n",
      "Recall   :  0.7143\n",
      "\n",
      "[[34  1]\n",
      " [ 2  5]]\n",
      "\n",
      "Evaluation for testing set\n",
      "Accuracy :  1.0\n",
      "F1       :  1.0\n",
      "Precision:  1.0\n",
      "Recall   :  1.0\n",
      "\n",
      "[[15  0]\n",
      " [ 0  3]]\n"
     ]
    }
   ],
   "source": [
    "#undersample\n",
    "virginica = df[df.virginica == 1].copy()\n",
    "not_virginica = df[df.virginica == 0 ].sample(frac=0.5).copy()\n",
    "df_undersample = pd.concat([virginica,not_virginica])\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(df_undersample.drop('virginica',axis=1), df_undersample.virginica, test_size=0.3,random_state=0)\n",
    "print(\"virginica in train set = \", str(y_train.sum()))\n",
    "print(\"virginica in dev set = \", str(y_dev.sum()))\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#Get predicted classes\n",
    "y_train_pred = logistic.predict_proba(X_train)[:,1]\n",
    "y_dev_pred = logistic.predict_proba(X_dev)[:,1]\n",
    "\n",
    "evaluation(y_train,y_train_pred,'training set')\n",
    "print()\n",
    "evaluation(y_dev,y_dev_pred,'testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try Generate Synthetic Samples\n",
    "\n",
    "### Try SMOTE - how it works:\n",
    "<img src=\"http://www.chioka.in/wp-content/uploads/2013/08/SMOTE.png\">\n",
    "\n",
    "### risk of SMOTE\n",
    "<img src=\"http://www.chioka.in/wp-content/uploads/2013/08/SMOTE-boundary.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginica in train set =  5\n",
      "virginica in dev set =  5\n",
      "\n",
      "AFTER SMOTE: virginica in train set =  36\n",
      "\n",
      "Evaluation for training set\n",
      "Accuracy :  0.9907\n",
      "F1       :  0.9863\n",
      "Precision:  0.973\n",
      "Recall   :  1.0\n",
      "\n",
      "[[71  1]\n",
      " [ 0 36]]\n",
      "\n",
      "Evaluation for testing set\n",
      "Accuracy :  0.9697\n",
      "F1       :  0.9091\n",
      "Precision:  0.8333\n",
      "Recall   :  1.0\n",
      "\n",
      "[[27  1]\n",
      " [ 0  5]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df.drop('virginica',axis=1), df.virginica, test_size=0.3,random_state=0)\n",
    "print(\"virginica in train set = \", str(y_train.sum()))\n",
    "print(\"virginica in dev set = \", str(y_dev.sum()))\n",
    "print()\n",
    "\n",
    "#smote\n",
    "sm = SMOTE(ratio=.5,k_neighbors =2,kind='regular',random_state=10);\n",
    "X_train, y_train = sm.fit_sample(X_train, np.ravel(y_train))\n",
    "\n",
    "print(\"AFTER SMOTE: virginica in train set = \", str(y_train.sum()))\n",
    "print()\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#Get predicted classes\n",
    "y_train_pred = logistic.predict_proba(X_train)[:,1]\n",
    "y_dev_pred = logistic.predict_proba(X_dev)[:,1]\n",
    "\n",
    "evaluation(y_train,y_train_pred,'training set')\n",
    "print()\n",
    "evaluation(y_dev,y_dev_pred,'testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try Penalized Models\n",
    "\n",
    "* You can build in a penalty proportional to the imbalance in the cost function\n",
    "* In NN or other algorithms you can define explicitly\n",
    "* In sklearn you can use the weighted function\n",
    "* In this case it will be similar to getting one TP wrong is equal to getting ~9 TN wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginica in train set =  5\n",
      "virginica in dev set =  5\n",
      "\n",
      "Evaluation for training set\n",
      "Accuracy :  0.9481\n",
      "F1       :  0.7143\n",
      "Precision:  0.5556\n",
      "Recall   :  1.0\n",
      "\n",
      "[[68  4]\n",
      " [ 0  5]]\n",
      "\n",
      "Evaluation for testing set\n",
      "Accuracy :  0.9091\n",
      "F1       :  0.7692\n",
      "Precision:  0.625\n",
      "Recall   :  1.0\n",
      "\n",
      "[[25  3]\n",
      " [ 0  5]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df.drop('virginica',axis=1), df.virginica, test_size=0.3,random_state=0)\n",
    "print(\"virginica in train set = \", str(y_train.sum()))\n",
    "print(\"virginica in dev set = \", str(y_dev.sum()))\n",
    "print()\n",
    "\n",
    "\n",
    "#logistic regression has class_weight - to penalize the cost function to be balanced\n",
    "logistic = LogisticRegression(class_weight='balanced')\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "#Get predicted classes\n",
    "y_train_pred = logistic.predict_proba(X_train)[:,1]\n",
    "y_dev_pred = logistic.predict_proba(X_dev)[:,1]\n",
    "\n",
    "evaluation(y_train,y_train_pred,'training set')\n",
    "print()\n",
    "evaluation(y_dev,y_dev_pred,'testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try Different Algorithms\n",
    "\n",
    "* Some algorithms are less suseptible to class imbalances such as tree based methods -- careful not to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for training set\n",
      "Accuracy :  0.987\n",
      "F1       :  0.8889\n",
      "Precision:  1.0\n",
      "Recall   :  0.8\n",
      "\n",
      "[[72  0]\n",
      " [ 1  4]]\n",
      "\n",
      "Evaluation for testing set\n",
      "Accuracy :  0.9697\n",
      "F1       :  0.8889\n",
      "Precision:  1.0\n",
      "Recall   :  0.8\n",
      "\n",
      "[[28  0]\n",
      " [ 1  4]]\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "#Get predicted classes\n",
    "y_train_pred = rfc.predict_proba(X_train)[:,1]\n",
    "y_dev_pred = rfc.predict_proba(X_dev)[:,1]\n",
    "\n",
    "evaluation(y_train,y_train_pred,'training set')\n",
    "print()\n",
    "evaluation(y_dev,y_dev_pred,'testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try a Different Perspective\n",
    "\n",
    "* There are fields of study dedicated to imbalanced datasets. They have their own algorithms, measures and terminology.\n",
    "\n",
    "* Taking a look and thinking about your problem from these perspectives can sometimes shame loose some ideas.\n",
    "\n",
    "* Two you might like to consider are anomaly detection and change detection.\n",
    "\n",
    "* checkout NFDS github from 2017-09 for examples on single class SVM for anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try Getting Creative\n",
    "\n",
    "* Really climb inside your problem and think about how to break it down into smaller problems that are more tractable.\n",
    "\n",
    "* For inspiration, take a look at the very creative answers on Quora in response to the question “In classification, how do you handle an unbalanced training set?”\n",
    "\n",
    "* Mix and match methods to find the right combination for your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you\n",
    "\n",
    "\n",
    "<img src=\"http://cdn.ttgtmedia.com/rms/microscopeuk/big%20data%20concept%202_290X230.jpg\">\n",
    "\n",
    "### Good luck exploring data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
